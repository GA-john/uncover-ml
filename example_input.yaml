
## This is a list of "feature sets", which are groups of tif files
## that can have different transforms
features:
  - name: featureset_1

    ## The type of the feature can be ordinal or categorical
    # type: categorical
    type: ordinal

    ## The sources of the files. Can be a directory (and all tifs in that 
    ## directory will be taken, a CSV list of paths, or a single path
    files:
      # - path: /path/to/some/file.tif
      # - list: /path/to/some/csv_file.csv
      - directory: /home/lb/data/GA-cover


    ## A list of transforms to apply to this featureset. They are 
    ## applied in order. See all possibilites below. Can be empty.
    transforms:
      ## "one-hot" encoding -- only works on categorical type features
      # - onehot 

      ## "one-hot" encoding with random projection to a lower dimensional
      ## subspace. Useful for the case of a covariate with many categories
      # - randomhot
      ## number of dimensions to project down to
      #     n_features: 10
      ## the random seed used to generate the projection
      #     seed: 1
      
      ## transform data to have mean 0
      - centre

      ## tranform data to have mean zero and std deviation 1 on all axes
      # - standardise

      ## transform data to have covariance matrix be the identity
      ## Also, only use the first keep_fraction of columns with the largest
      ## variance to reduce noise
      # - whiten:
      #   keep_fraction: 0.8
    
    ## The method used to impute missing data.
    ## No imputation
    imputation: none

    ## Impute by replacing missing data with the mean of that band
    # imputation: mean

    ## Use a Gaussian (ie linear) model to predict the missing values
    # imputation: gauss

    ## Fill with values from the nearest-neighbour in *feature space*
    # imputation: nn


  ## Have as many feature-sets as you like
  - name: featureset_2
    type: categorical
    files:
      - directory: /some/other/data
    transforms:
      - onehot
    imputation: none
  

## These are another set of transforms that are applied to all the feature
## sets together after they have been concatenated into a single feature vector.
## Makes a lot of sense to do imputation at this stage. The same transforms
## are available (except one-hot encoding, which has to happen in the inital
## feature sets
preprocessing:
  imputation: nn
  transforms:
    - whiten:
      keep_fraction: 0.8

targets:
  ## Path the target shapefile
  file: /path/to/shapefile.shp
  ## the property to model from the above shapefile
  property: calcium_level

## Options for clustering. This is an unsupersised or semi-supervised 
## alternative to (supervised) regression
clustering:
  ## OPTIONAL path to class data for the semi-supervised case
  file: /home/lb/data/GA-cover/geochem_sites_class1.shp
  ## The property with class data in the above file. Expecting integer classes
  ## greater than 0
  property: class
  ## Algorithm for clustering: Currently only kmeans implemented
  algorithm: kmeans
  arguments:
    ## The number of classes to cluster the data into
    n_classes: 12
    ## Parameter of the initialisation algorithm. See doco for more details
    ## but a high number means slower/more memory, but better initialisation.
    ## Scales with nodes so set to one for > 16 nodes or so.
    oversample_factor: 5

## Options for supervised learning
learning:
    ## Algorithm used for learning. See the documentation for the many 
    ## possible options here
    algorithm: svr
    ## Arguments to the algorithms -- these change for each algorithm
    ## so you'll need to check the documentation to see what you need here
    arguments:
      target_transform: rank

## Options for the prediction step
## Applies to both clustering and regression
prediction:
  ## IF you've selected a probabilistic algorithm, this will provide the
  ## upper and lower quantiles specificed as additional outputs
  quantiles: 0.95

## Validation options in the learning phase
validation:
  ## rank each feature via a subtractive leave-one-out method
  #- feature_rank
  ## compute a variety of performance scores using k-fold cross-validation
  - k-fold:
      ## number of folds into which the data is split
      folds: 5
      ## The seed for generating the folds. Keep constant so different runs
      ## get exactly the same folds
      random_seed: 1
  
output:
  ## Directory to output results
  directory: .

