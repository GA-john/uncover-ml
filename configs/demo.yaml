
# not currently implemented
# patchsize: 0

features:
  - name: my pickle files
    type: pickle
    files:
      covariates: /home/sb/data/GA-cover/features.pk
      targets: /home/sb/data/GA-cover/targets.pk
      rawcovariates: /home/sb/data/GA-cover/rawcovariates.csv
      rawcovariates_mask: rawcovariates_mask.csv

  - name: my features 1
    type: ordinal
    files:
      # - path: /home/lb/data/GA-cover/Th_v1.tif
      # - list: /home/lb/data/GA_Cover2_sirsam_LLC/sirsam_covariates_convolved.txt
      - directory: /home/lb/data/GA-cover
    # transforms are performed in order
    transforms:
      - centre
      - standardise
    imputation: none
  
  - name: my features 2
    type: categorical
    files:
      # - path: /home/lb/data/GA-cover/Th_v1.tif
      # - list: /path/to/csv_file.csv
      - directory: /home/lb/data/GA-cover/cat_datasets
    # transforms are performed in order
    transforms:
      - randomhot:
          n_features: 10
          seed: 1
    imputation: none

# mask file used for masking prediction grid
# Mask with values=retain will be predicted
mask:
  file: /home/lb/data/GA-cover/mask/old_mask_test.tif
  retain: 1

preprocessing:
  # imputation: none
  # imputation: gaus
  # imputation: nn
  imputation: mean

  transforms:
    # - whiten:
        # keep_fraction: 0.8

# resample: choice (value, spatial)
# bootstrap: bool, sampling with or without replacement
# output_samples: number of output samples kept
# bins: for value based resampling only. Number of bins to sample from.
# rows and cols: for spatial resampling, sample from rows X cols tiles

targets:
  file: /home/lb/data/GA-cover/geochem_sites.shp
  property: K_ppm_imp_
  resample: value
  arguments:
    bins: 10
    bootstrap: True
    output_samples: 500
#  resample: spatial
#  arguments:
#    rows: 4
#    cols: 4
#    bootstrap: True
#    output_samples: 500

clustering:
  file: /home/lb/data/GA-cover/geochem_sites_class1.shp
  property: class
  algorithm: kmeans
  arguments:
    n_classes: 12
    oversample_factor: 5

# target_transform: target transform (choice: stardardise, sqrt, log, identity,
# logistic, rank, kde)

learning:
    # algorithm: sgdapproxgp
    # arguments:
    #     target_transform: standardise
    #     kern: rbf
    #     lenscale: 1000
    #     ard: True
    #     maxiter: 5000
    #     nbases: 50
    #     random_state: 1
    algorithm: randomforest
    arguments:
      n_estimators: 500
      target_transform: standardise

#    algorithm: multirandomforest
#    arguments:
#       n_estimators: 50
#       target_transform: standardise
#       forests: 10
#       parallel: True
#       outdir: .


#n_components : int, float, None or string
#        Number of components to keep.
#        if n_components is not set all components are kept::
#        if n_components == 'mle' and svd_solver == 'full', Minka\'s MLE is used
#        to guess the dimension
#        if ``0 < n_components < 1`` and svd_solver == 'full', select the number
#        of components such that the amount of variance that needs to be
#        explained is greater than the percentage specified by n_components
#        n_components cannot be equal to n_features for svd_solver == 'arpack'.
# svd_solver: {'auto', 'full', 'arpack', 'randomized'}
# iterated power: int > 0 or 'auto'
# kde transform is not working yet

optimisation:
  algorithm: randomforest
  featuretransforms:
    pca:
      n_components: [0.5, 0.8]
      whiten: [False, True]
      svd_solver: ['auto']
      iterated_power: ['auto']
  hyperparameters:
    n_estimators: [10, 50, 200]
    target_transform: ['identity', 'standardise', 'log', 'rank', 'logistic']
  optimisation_output: optimisation.csv


prediction:
  quantiles: 0.95

validation:
  #- feature_rank
  - parallel
  - k-fold:
      folds: 5
      random_seed: 1
  
output:
  directory: .

